%!TEX root = ../Thesis.tex
\chapter{Introduction}
In this dissertation, we consider the \emph{design and analysis of algorithms and data structures}:

%The core of the field of \emph{design and analysis of algorithms and data structures} is as follows:

\begin{leftbar}
%\paragraph{Design and analysis of algorithms and data structures}
\noindent Our goal is to use computational resources efficiently. We \emph{design} data structures and algorithms that solve \emph{problems}, and \emph{analyse} proposed \emph{solutions} in a \emph{model of computation} that lets us predict efficiency and compare different solutions on real computers.
\end{leftbar}

\emph{Data Structures} are the basic building blocks in software engineering; they are the organizational method we use to store and access information in our computers efficiently. 
An \emph{Algorithm} specifies the steps we perform to complete some task on an input in order to produce the correct output, relying on underlying data structures. %, typically relying on underlying data structures. 
Naturally, deep knowledge and understanding of algorithms and data structures are core competences for software engineers, absolutely vital to developing efficient software. 

This dissertation documents our research in this field of theoretical computer science, and later chapters each include one of the following papers that are published in or submitted to peer-reviewed conference proceedings. 
Papers appear in separate chapters in their original form except for formatting, meaning that notation, language and terminology has not been changed and may not be consistent across chapters. 
%The chapter titles are equal to the original paper titles. 
Authors are listed alphabetically as is the tradition in theoretical computer science (except for \emph{Indexing Motion Detection Data for Surveillance Video}, which was published at a multimedia conference). 

%For software engineers, knowledge and understanding of algorithms and data structures remain a core competence, absolutely vital to developing efficient and predictable software. This is especially true for data structures, as they are the basic building blocks in software engineering; they are the method through which we store and access information in our computers. 

\begin{description}
    \item[Chapter \ref{chp:fingerprints}: \emph{Fingerprints in Compressed Strings}.]~\\
    By Philip Bille, Patrick Hagge Cording, Inge Li Gørtz, Benjamin Sach, Hjalte Wedel Vildhøj and Søren Vind. At Algorithms and Data Structures Symposium (WADS) 2013~\cite{bille2013fingerprints}.
    \item[Chapter \ref{chp:dynamiccompression}: \emph{Dynamic Relative Compression and Dynamic Partial Sums}.]~\\
    By Philip Bille, Patrick Hagge Cording, Inge Li Gørtz, Frederik Rye Skjoldjensen, Hjalte Wedel Vildhøj and Søren Vind. In submission.
    \item[Chapter \ref{chp:annotated}: \emph{Compressed Pattern Matching in the Annotated Streaming Model}.]~\\
    By Markus Jalsenius, Benjamin Sach and Søren Vind. In submission.
    \item[Chapter \ref{chp:patternextraction}: \emph{Output-Sensitive Pattern Extraction in Sequences}.]~\\
    By Roberto Grossi, Giulia Menconi, Nadia Pisanti, Roberto Trani and Søren Vind. At Foundations of Software Technology and Theoretical Computer Science (FSTTCS) 2014~\cite{grossi2014output}.
    \item[Chapter \ref{chp:compressedrangesearching}: \emph{Compressed Data Structures for Range Searching}.]~\\
    By Philip Bille, Inge Li Gørtz and Søren Vind. At International Conference on Language and Automata Theory and Applications (LATA) 2015~\cite{bille2015compressedrs}.
    \item[Chapter \ref{chp:coloredrangesearching}: \emph{Colored Range Searching In Linear Space}.]~\\
    By Roberto Grossi and Søren Vind. At Scandinavian Symposium and Workshops on Algorithm Theory (SWAT) 2014~\cite{grossi2014colored}.
    \item[Chapter \ref{chp:motiondetection}: \emph{Indexing Motion Detection Data for Surveillance Video}.]~\\
    By Søren Vind, Philip Bille and Inge Li Gørtz. At IEEE International Symposium on Multimedia (ISM) 2014~\cite{vind2014indexing}.
\end{description}

The journal version of the following paper appeared during my PhD enrollment, but as the results were obtained and the conference version published prior to starting the PhD programme, the paper is omitted from this dissertation.

\begin{description}
    \item[\emph{String Indexing for Patterns with Wildcards}.]~\\
    By Philip Bille, Inge Li Gørtz, Hjalte Wedel Vildhøj and Søren Vind. At Scandinavian Symposium and Workshops on Algorithm Theory (SWAT) 2012~\cite{bille2012string}. In Theory of Computing Systems 2014~\cite{bille2014string}.
\end{description}

The dissertation is organized as follows. 
The following Section \ref{sec:in-back} is a brief general introduction to the field of algorithmic research, and may be skipped by readers familiar with the subject. 
Chapter \ref{chp:contributions} gives an overview of our contributions, and the later Chapters include the papers mentioned above.


%How does the world look? And where are we?
\section{Background and Context}\label{sec:in-back}
%\section{Algorithms and Data Structures: Taxonomy} 
%The academic field of Computer Science emerged from mathematics and electrical engineering during the latter half of the 20th century. 
%The first large-scale use of computers was during the Second World War, where they were used to break encryption schemes. Ever more complex and large computer installations were built in the following years, primarily for military use, but the machines eventually found a home in industry and universities. 
%The result was that the first academic institutions devoted to studying computer science were created almost exactly fifty years ago (e.g. The Department of Computer Science at Carnegie Mellon University was founded in 1965, as possibly the first such department in the world).

In his pioneering work \emph{``On Computable Numbers, with an Application to the Entscheidungsproblem''}~\cite{turing1936computable} from 1936, Alan Turing in a single stroke became the father of the field of \emph{theoretical computer science}. He gave the first general model of the theoretical capabilities of computers with the \emph{Turing Machine}, and in doing so formalized the concept of \emph{algorithms}.
In the following 80 years the computer revolution occurred and the first institutions devoted to computer science research were established around fifty years ago, causing the field to naturally expand\footnote{In 1965, the Department of Computer Science was founded at Carnegie Mellon University, as possibly the first such department in the world.}. 
%\fix{Move to after defs below} The fields of algorithms and data structures are extremely closely related, and we often use solutions and ideas from one in the other. 
Today, the field is founded on modern day equivalents of the concepts introduced by Turing:

\begin{description}
    \item[Algorithms] 
    An \emph{algorithm} is a method for solving a \emph{problem} on an input which produces the correct output. 
%The method can be thought of as a series of calculations that must be performed on the input in order to generate the output. 
    The problem is a central concept which states the properties of the input and requirements for the output.
    \prob{Sorting} is a classic example of an algorithmic problem: given as input a list of numbers, return the same numbers listed in non-decreasing order.
    \item[Data Structures] 
    A \emph{data structure} is a method for maintaining a representation of some data while supporting \emph{operations} on the data. Operations typically either \emph{update} or answer \emph{queries} on the stored data. We may think of a data structure as a collection of algorithms operate on the stored data.
    An example of a data structure problem is \prob{sorted list representation}: store a list of numbers subject to insertions and deletions, and support queries for the $k$'th smallest number.
    \item[Model of Computation] 
    The \emph{model of computation}\footnote{Also sometimes called the \emph{machine model}.} is an abstract model of a computer that ignores most real-world implementation details. This allows us to reason about the performance of algorithms and data structures on real physical computers.
    The model of computation we use in this thesis is called the \emph{unit-cost $w$-bit Word RAM}, which resembles the capabilities of modern day processors: 
    memory is modeled as a sequence of $w$-bit words that can be read and written, and we can perform arithmetic and word-operations on (pairs of) words.
\end{description}

Generally, research in a particular problem takes two different angles. 
One is to prove that the problem is impossible to solve using fewer resources than some \emph{lower bound}, meaning that \emph{any} solution must use at least that many resources. However, in this thesis our focus is on the opposite direction, which is to show that there is a solution with some \emph{upper bound} on the resources used.

We characterise proposed solutions using several parameters.
%, including \emph{correctness} and \emph{performance}. 
The output of a (deterministic) solution must always adhere to its requirements and be a \emph{correct} answer.
The \emph{performance} of a solution is the amount of resources it requires in our model of computation relative to the size $n$ of a finite input. Typically, we split the performance into two components: the \emph{space usage} is the number of memory words required, and the \emph{time} is the number of unit-cost machine operations required. Here, we study \emph{worst-case analysis}, which is the performance obtained when given an input that causes the solution to perform as poorly as possible.

There are some problems for which traditional \emph{deterministic} solutions have inherently bad worst-case performance. To work around this, \emph{randomized} solutions relaxes the strict requirements on correctness or good worst-case performance. Instead, \emph{Monte Carlo} solutions have worst-case performance guarantees with a low probability of producing incorrect outputs. On the contrary, \emph{Las Vegas} solutions always produce correct outputs with a low risk of exhibiting bad performance for some inputs.

\section{Why is this Important?}
We now give a slightly contrived example that serves to illustrate why studying algorithms and data structures is important. It shows two different possible solutions for the fundamental \emph{dictionary} data structure problem, and indicates how we can calculate their (relative) performance. 

\begin{leftbar}
    %\vspace{-1.4em} \paragraph{A \prob{Dictionary} Data Structure Example: Why are data structures important?}~\\
    Imagine that you want to look up the word \emph{``Dissertation''} in The Oxford English Dictionary in the physical book version from 1989. 
    You remember that you used to perform this kind of query before the internet, so you would know to go to a page roughly in the middle and see if you had surpassed the entries for ``D''. If so, you would go to the middle part in the left half and continue like that. The property of the dictionary you use is that dictionary editors traditionally sort the entries. 
    
    However, suppose that the editors forgot to sort the words before printing your dictionary: How would you find the entry? Your only possibility would be to start on the first page and check each entry it contains. This is unfortunate: You risk that the entry is on the last page in the dictionary, forcing you to look at all pages!
    
    The 1989 Oxford English Dictionary consist of $20$ volumes with $21 730$ pages that contain about $300 000$ entries. If you can check $1$ entry per second the normal (sorted) search would complete in less than $20$ seconds, but the second (unsorted) search would require slightly more than $83$ hours. That is, your normal search is roughly $16000$ times faster than the unsorted search!
    
    Our sorted or unsorted dictionary is a good model of how we may store data in our computer, and the search for a word is called a \emph{membership} query. In computer science the first (sorted) search is called a \emph{binary search}, and it can be performed exactly when the data we search in is sorted. Otherwise we must perform a \emph{linear search} like the second (unsorted) search. 

    \begin{comment}
    %We here give a description of a simplified data structure for the \prob{membership} problem, which is to store a set of distinct numbers $X$ subject to member queries: given a number $y$, does $X$ contain $y$? 
    We now give an example that serves to illustrate how we can reason about the performance of two different possible solutions when solving a fundamental data structure problem. The problem is called \prob{membership}, and is to
    %for illustrative purposes. Our aim here is not to give the most efficient solution.    
    %The \prob{membership} problem is to 
    store a set of numbers $N$ and support \emph{member} queries on $N$: given a number $y$, does $N$ contain $y$? We analyse the time spent answering a query by counting how many number comparisons are made. 
    %\fix{we ignore space and construction time}
    
    Our first proposed solution is to store the unsorted numbers $N$ in a list. To answer the query, we start at one end of the list and compare $y$ with a number $x$ in the list: if $x = y$ we answer $\YES$ (since then $N$ contains $y$); otherwise we move to the next number in the list and compare it to $y$ as before. If we reach the end of the list without finding $y$, then $y$ is not in $N$ and we answer $\NO$. 

%    Observe that answering a query always requires as many comparisons as the size of the list if $y \not\in X$, which we write as $\Oh(|X|)$.
 
    An alternative solution is to store the numbers $N$ in a list sorted in increasing order, meaning that the smallest number is first in the list and the largest number is last in the list.
    This allows us to repeatedly reduce the size of the part of the list where $y$ can possibly exist by half. The technique is called \emph{binary search}, and  %compare $y$ to an element and exploit the ordering to ignore half of the list where $y$ cannot possibly lie. %using a single comparison.%and thus narrow down the search area much more efficiently than one element at a time.
    it works as follows. We find the middle number $m$ in the list, and observe that $m$ partitions the numbers in the list depending on how they compare to $m$: all smaller numbers are left of $m$ and all larger numbers are right of $m$.
    %No matter the result, the ordering means that $y$ can only possibly be in one half of the list and we can thus ignore the other half: 
    This means that if $y$ is smaller than $m$, the only possible positions where $y$ can appear (if it is in $N$) are in the left half of the list. If $y$ is larger than $m$ then $y$ can appear only in the right half.
    %This means that if $y$ is smaller than $m$ we never need to look for $y$ to the right of $m$, so we ignore the upper half of the list (and if $y$ is larger than $m$ we ignore the lower half). 
    %Thus, if $m < y$ we ignore the lower half of the list, and conversely if $m > y$ we ignore the upper half. 
    
    The search starts by comparing $m$ to $y$. If $m = y$ we answer $\YES$ as then $N$ contains $y$. Otherwise if $m < y$ we ignore the left half of the list, and conversely if $y < m$ we ignore the right half. 
    We continue the search in the same way, by repeatedly comparing $y$ to the middle number of the non-ignored part of the list. In this way, we ignore half of the remaining list until the non-ignored list only contains one number $r$. If $r = y$ we answer $\YES$, and otherwise we answer $\NO$. 
    
    The worst case performance occur when $N$ does not contain $y$. Here, the first solution compares $y$ with all numbers in $N$, so the number of comparisons performed equals how many numbers are in $N$, which we write $|N|$. The alternative solution repeatedly use a single comparison to halve the size of the non-ignored part of the list until it reaches a size of $1$. Since $\log |N|$ is the number of halvings of $|N|$ until it reaches a size of at most $1$, we perform $\log |N|$ comparisons in the second solution.
        
    In the above descriptions we ignored the time spent constructing the data structures. This can be done in $\Oh(|X|)$ time for the unsorted list and $\Oh(|X| \log |X|)$ time for the sorted solution. As the list is only created once and then queried repeatedly, the sorted solution spend less time in total if we make more than a constant number of queries.    
    \end{comment}
\end{leftbar}

Structuring data in an ordered fashion to help search is of course not a new invention from computer science. The alphabetic ordering of information dates back at least two thousand years, and the first monolingual English dictionary from 1604 was also ordered alphabetically\footnote{Interestingly, the alphabetic ordering was unusual enough to require an explanation from the editor.}. The example simply shows two simple possibilities for structuring data: the remainder of this dissertation discuss much more advanced data structuring techniques and algorithms for finding information in the structured data. As shown in the example, the difference in performance is dramatic, and it only grows with larger data sets. This is the main motivation for studying algorithms and data structures: we \emph{have to} use clever algorithmic techniques to cope with the ever growing quantities of data.


\begin{comment}
\begin{leftbar}
    \vspace{-1.4em} \paragraph{Lower Bound Example: \prob{Sorting}.} \todo{Lower Bound example, motivation}
    %Consider the classic problem of sorting a list of $n$ numbers where we measure the number of comparisons and swaps of pairs of numbers in the list. 
    %There is a lower bound showing that $\Omega(n \log n)$ comparisons are necessary\docite{sorting}, and a matching upper bound giving an algorithm for solving the problem using $\Oh(n \log n)$ comparisons\docite{mergesort, heapsort}. 
    It is easy to show a lower bound of $\Omega(n \log n)$ for \prob{sorting} $n$ numbers using comparisons and swaps of pairs of numbers. If all $n$ numbers are distinct they have $n!$ possible permutations, only one of which is the correct ordering. Then the number of number-pair comparisons to identify a single permutation uniquely is $\Omega(\log (n!)) = \Omega(n \log n)$. There are several matching upper bounds in the form of algorithms solving the problem using $\Oh(n \log n)$ comparisons\docite{mergesort, heapsort}.

    Observe that the sorting lower bound immediately implies a lower bound on the time per operation for any \prob{sorted list representation} data structure $\mathcal{D}$. The argument is as follows: First insert the entire list of $n$ numbers into $\mathcal{D}$, and then repeatedly get, store, and remove the smallest number in $\mathcal{D}$. Then the result is an ordered version of the original list, created in $\Oh(n)$ operations, meaning that at least one of the operations must take time $\Omega(\log n)$.

    Generally, lower bounds in weaker models may be circumvented by a stronger machine model. For example, there is a sorting algorithm taking $\Oh(n \log \log n)$ time in the Word RAM model.
\end{leftbar}
\end{comment}


