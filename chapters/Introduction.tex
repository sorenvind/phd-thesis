%!TEX root = ../Thesis.tex
\chapter{Introduction}
\emph{Data Structures} are the basic building blocks in software engineering; they are the method we use to organise and access information in our computers. An \emph{Algorithm} specifies the steps we take in order to perform some task on an input in order to produce a given output, often relying on underlying data structures. Naturally, deep knowledge and understanding of algorithms and data structures is a core competence for software engineers, absolutely vital to developing efficient and predictable software.

%For software engineers, knowledge and understanding of algorithms and data structures remain a core competence, absolutely vital to developing efficient and predictable software. This is especially true for data structures, as they are the basic building blocks in software engineering; they are the method through which we store and access information in our computers. 

In this chapter, I will give a brief primer to the study of data structures and an overview of the important problems in the field and introduce the included papers.

%How does the world look? And where are we?
\section{Bits of Background and Context}
%\section{Algorithms and Data Structures: Taxonomy} 
The academic field of Computer Science emerged from mathematics and electrical engineering during the latter half of the 20th century. 
The first large-scale use of computers was during the Second World War, where they were used to break encryption schemes. Ever more complex and large computer installations were built in the following years, primarily for military use, but the machines eventually found a home in industry and universities. 
The result was that the first academic institutions devoted to studying computer science were created almost exactly fifty years ago (e.g. The Department of Computer Science at Carnegie Mellon University was founded in 1965, as possibly the first such department in the world).

In his pioneering work \emph{``On Computable Numbers, with an Application to the Entscheidungsproblem''}~\cite{turing1936computable} from 1936, Alan Turing in a single stroke became the father of the field of \emph{theoretical computer science}. He gave the first general model of the theoretical capabilities of computers with the \emph{Turing Machine}
%, that is the basis of all later theoretical computer models
, and (among others) gave a formalisation of \emph{algorithms}.
In the following 80 years, the field expanded in various directions, but the design and analysis of \emph{algorithms} and \emph{data structures} remain at the core. 

%\fix{Move to after defs below} The fields of algorithms and data structures are extremely closely related, and we often use solutions and ideas from one in the other. 
In the following, we describe the fields of algorithms and data structures as one, and give a very brief introduction and taxonomy of the basic concepts as they are now established.
The modern equivalents of the concepts introduced by Turing are as follows:

\begin{description}
    \item[Machine Model] We use an abstract model of a computer that ignores most details and allows us to understand its behaviour and reason about performance.
    For example, computation in the very common Word RAM model resembles the capabilities of modern day CPUs: memory is modeled as a sequence of words with $w$ bits. We can read or write a word in unit time and perform arithmetic and word-operations on a constant number of words in unit time.
    \item[Algorithms] In its most general formulation, an algorithm is a method for solving a problem on an input that produces the correct output. 
%The method can be thought of as a series of calculations that must be performed on the input in order to generate the output. 
    The \emph{problem} is a central concept which states the desired properties of the input and output.
    One classic example of an algorithmic problem is \underline{sorting}: given as input a list of numbers, return the same numbers in non-decreasing order.
    \item[Data Structures] A data structure is a method for maintaining a representation of some data while supporting a set of \emph{operations} on the data: allow \emph{updates}, and answer \emph{queries} on the updated data. 
    An example of a data structure problem is \underline{sorted list representation}: store a list of numbers subject to insertions and deletions, and support queries for the $k$'th smallest number.
\end{description}

%\paragraph{Solution Qualities}
We characterise proposed solutions using several parameters.
%, including \emph{correctness} and \emph{performance}. 
First, the output of an algorithm or query must always be (one of the possible) \emph{correct} answers.
The \emph{performance} of a solution is the amount of resources required to execute it in our machine model, calculated relative to the size $n$ of a finite input: the \emph{space usage} is the number of words (or bits) required, and the \emph{time} is the number of machine operations necessary. We generally consider \emph{worst-case analysis}, meaning the performance obtained when given the input that cause the solution to perform as poorly as possible.

%Predictability is another important parameter that characterises how the solution performs on different inputs. We are mostly worried with \emph{worst-case analysis}, meaning the performance obtained when given the input that cause the solution to perform as poorly as possible.

Traditional \emph{deterministic} solutions have inherently bad worst-case performance when solving some problems. To this end, \emph{randomized} solutions relaxes the strict requirements on correctness and worst-case analysis. Instead, \emph{monte carlo} solutions have worst case performance guarantees but some probability of giving an incorrect output. On the contrary, \emph{las vegas} solutions always give a correct output, but with a risk of bad performance.

%\paragraph{Upper and Lower Bounds}
Generally, research in a particular problem takes two different angles. 
One is to prove that it is impossible to solve the problem using less resources than some \emph{lower bound} for \emph{any} solution in a given machine model. The opposite direction is to show the existence of a solution where the amount of used resources can be limited by some \emph{upper bound}. 
%That is, on one hand we try to show that we cannot possibly do better, and on the other we show what we can actually do. 

%One is to prove \emph{lower bounds}, showing that it is impossible to solve the problem using less resources than some lower bound in a given machine model. When proving \emph{upper bounds}, we do the opposite by showing the existence of a solution where the amount of used resources is limited by some upper bound. 
\paragraph{The Classic Example of Sorting}
%Consider the classic problem of sorting a list of $n$ numbers where we measure the number of comparisons and swaps of pairs of numbers in the list. 
%There is a lower bound showing that $\Omega(n \log n)$ comparisons are necessary\docite{sorting}, and a matching upper bound giving an algorithm for solving the problem using $O(n \log n)$ comparisons\docite{mergesort, heapsort}. 
It is easy to show a lower bound of $\Omega(n \log n)$ for sorting $n$ numbers using comparisons and swaps of pairs of numbers: If all $n$ numbers are distinct they have $n!$ possible permutations, only one of which is the correct ordering. Then the number of number-pair comparisons to identify a single permutation uniquely is $\Omega(\log (n!)) = \Omega(n \log n)$. There are several matching upper bounds in the form of algorithms solving the problem using $O(n \log n)$ comparisons\docite{mergesort, heapsort}.

Observe that the sorting lower bound immediately implies a lower bound for any sorted list representation data structure $\mathcal{D}$. The argument is as follows: First insert the entire list of $n$ numbers into $\mathcal{D}$, and then repeatedly get, store, and remove the smallest number in $\mathcal{D}$. Then the result is an ordered version of the original list, created in $O(n)$ operations, meaning that at least one of the operations must take time $\Omega(\log n)$.

However, the lower bounds does not apply in the Word RAM model where there \emph{is} a linear ($O(n)$) time solution if the numbers each fit in a word. That is, generally lower bounds in weaker models may be circumvented by a stronger machine model.

% So what particular branch do we consider? And why is this interesting?
\section{My Particular Branch: What and Why?}
Clearly, although I have only touched upon major concepts there are many possible research directions to take. My main interest is in designing data structures from a theoretical perspective in order to obtain new and improved upper bounds compared to previously known results. I have particularly enjoyed considering the simplest of questions, focusing on core data structure primitives and extremely classic problems.

A common theme in my work and a particular interest of mine is to find solutions that require relatively little space. Each of the proposed data structures exhibit this particular feature, though they span several different classic subfields of algorithmic research, namely combinatorial pattern matching, geometry, and compression. 

The papers included in the latter chapters of this thesis each contribute at least one non-trivial newly-designed data structure for solving a particular problem. In some instances, the data structure is used as a cornerstone in giving a solution to a particular algorithmic problem, while in other cases the data structure is the singular contribution. All but one of the papers are purely theoretical, meaning that although the proposed solutions may be good in practice, their efficiency have only been established in theory. In a single case, the proposed data structure was implemented and tested experimentally in practical scenarios.

As initially stated, I consider data structures to be some of the most fundamental building blocks for efficiently solving problems. To see why, consider the following example from the origins of the field of combinatorial pattern matching. In 1970, a fair amount of research had gone into finding solutions to the problem of determining the \emph{longest common substring}\footnote{Given two strings of length $n$, what is the longest substring that is in both strings?} of two strings of length $n$, and thus it was conjectured that no linear time solution existed\docite{donald knuth}. However, in 1973 Peter Weiner~\cite{weiner1973linear} disproved the conjecture by giving a linear time algorithm for building the \emph{suffix tree} data structure, which immediately solved the problem (and many others). In the following 40 years, the properties of the data structure and its extensions have been exploited to solve countless problems\docite{many papers} in combinatorial pattern matching, where it remains a key primitive.



\section{Background: A Data Structure Primer}

\begin{itemize}
    \item Models (Pointer Machine, Word Ram, Streaming)
        - Theory vs Practice
        - Massive Data Sets
    \item Basics of Data Structures
        - Trees and Tries
        - Predecessor
    \item Strings
        - Suffix Tree
        - Fingerprints
    \item Geometric
        - Orthogonal Range Searching
        - With Colors
        - And Motion
    \item Compression
        - DAG compression
        - SLPs and derivatives
\end{itemize}



\section{An Overview and Outline of Results}
The remaining chapters of this dissertation include the following papers, each of them submitted to or published in peer-reviewed conference proceedings. The papers appear in their original form, meaning that notation, language and terminology has not been changed and may not be consistent across chapters. The chapter titles are equal to the original paper titles. Authors are listed alphabetically as is the tradition in the field (except for the paper \emph{Indexing Motion Detection Data for Surveillance Video} as it was published at a multimedia conference where that is not the norm).

\begin{description}
    \item[Fingerprints in Compressed Strings.] By Philip Bille, Patrick Hagge Cording, Inge Li Gørtz, Benjamin Sach, Hjalte Wedel Vildhøj and Søren Vind. Presented at Algorithms and Data Structures Symposium (WADS), 2013.
    \item[Colored Range Searching In Linear Space.] By Roberto Grossi and Søren Vind. Presented at Scandinavian Symposium and Workshops on Algorithm Theory (SWAT), 2014.
    \item[Indexing Motion Detection Data for Surveillance Video.] By Philip Bille, Inge Li Gørtz and Søren Vind. Presented at IEEE International Symposium on Multimedia (ISM), 2014.
    \item[Output-Sensitive Pattern Extraction in Sequences.] By Roberto Grossi, Giulia Menconi, Nadia Pisanti, Roberto Trani and Søren Vind. Presented at Foundations of Software Technology and Theoretical Computer Science (FSTTCS), 2014.
    \item[Compressed Data Structures for Range Searching.] By Philip Bille, Inge Li Gørtz and Søren Vind. Presented at International Conference on Language and Automata Theory and Applications (LATA), 2015.
    \item[Annotated Data Streams with Multiple Queries.] By Markus Jalsenius, Benjamin Sach and Søren Vind. In submission.
    \item[Dynamic Relative Compression] By Philip Bille, Patrick Hagge Cording, Inge Li Gørtz, Frederik Rye Skjoldjensen, Hjalte Wedel Vildhøj and Søren Vind. In submission.
\end{description}

The journal version of the following paper appeared during my PhD, but as the results were obtained and the conference version published prior to starting the PhD programme the paper is omitted from this dissertation.

\begin{description}
    \item[String Indexing for Patterns with Wildcards.] By Philip Bille, Inge Li Gørtz, Hjalte Wedel Vildhøj and Søren Vind. Presented at Scandinavian Symposium and Workshops on Algorithm Theory (SWAT), 2012. In Theory of Computing Systems, 2014. 
\end{description}

For each paper or subject considered in the later chapters:
\begin{itemize}
    \item Our Contributions
    \item Future Directions
\end{itemize}

